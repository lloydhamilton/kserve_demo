{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deploying a model using KServe\n",
    "\n",
    "## Prerequisites\n",
    "You will need to have a cluster with kserve installed. If you do not have a cluster, you can create one by following the guide [here](https://medium.com/towards-data-science/kserve-highly-scalable-machine-learning-deployment-with-kubernetes-aa7af0b71202)\n",
    "\n",
    "## Introduction\n",
    "This notebook demonstrates how to deploy a model using KServe. We will be deploying two models, a simple sklearn model and a custom model.\n",
    "\n",
    "Deploy sklearn model with the following command:\n",
    "```bash\n",
    "kubectl apply -n kserve -f - <<EOF\n",
    "apiVersion: \"serving.kserve.io/v1beta1\"\n",
    "kind: \"InferenceService\"\n",
    "metadata:\n",
    "  name: \"sklearn-iris\"\n",
    "spec:\n",
    "  predictor:\n",
    "    model:\n",
    "      modelFormat:\n",
    "        name: sklearn\n",
    "      storageUri: \"gs://kfserving-examples/models/sklearn/1.0/model\"\n",
    "EOF\n",
    "```\n",
    "\n",
    "Ensure that you have port forwarded the istio-ingressgateway service to access the model. You can do this by running the following command:\n",
    "```bash\n",
    "kubectl port-forward -n istio-system svc/istio-ingressgateway 8080:80\n",
    "```\n",
    "\n",
    "Then send a request to the model using cell below."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "'{\"predictions\":[1,1]}'"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing for sklearn iris model\n",
    "import requests\n",
    "\n",
    "url = \"http://localhost:8080/v1/models/sklearn-iris:predict\"\n",
    "service_host_name = \"sklearn-iris.kserve.example.com\"\n",
    "request_headers = {\n",
    "    \"Host\": service_host_name\n",
    "}\n",
    "payload = {\n",
    "    \"instances\": [\n",
    "        [6.8,  2.8,  4.8,  1.4],\n",
    "        [6.0,  3.4,  4.5,  1.6]\n",
    "    ]\n",
    "}\n",
    "response = requests.post(url, json=payload, headers=request_headers)\n",
    "response.text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deploying a custom predictor\n",
    "To deploy a custom predictor, you will need to create a docker image and push it to a container registry. This repository already has a custom predictor that you can use leverages continuous interation to build in github workflows.\n",
    "\n",
    "To apply the latest version of the model, run the following command:\n",
    "```bash\n",
    "kubectl apply -f custom_predictor/deployment/custom_predictor.yaml\n",
    "```\n",
    "\n",
    "Ensure that you have port forwarded the istio-ingressgateway service to access the model. You can do this by running the following command:\n",
    "```bash\n",
    "kubectl port-forward -n istio-system svc/istio-ingressgateway 8080:80\n",
    "```\n",
    "\n",
    "Then send the request to the model using the cell below.\n",
    "\n",
    "**Note: The custom predictor is a dummy model that returns a random tensor with shape (3, 360, 640) for any input.**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "import requests\n",
    "import base64\n",
    "import io\n",
    "import uuid\n",
    "from PIL import Image\n",
    "from src.data_models import InferenceV2Inputs, InferenceV2\n",
    "\n",
    "def bytes_to_json_serializable(bytes_data: bytes) -> str:\n",
    "    return base64.b64encode(bytes_data).decode(\"utf-8\")\n",
    "\n",
    "# read pil image to bytes\n",
    "with io.BytesIO() as output:\n",
    "    with Image.open(\"cat.jpg\") as img:\n",
    "        img.save(output, format=\"PNG\")\n",
    "        image_size = img.size\n",
    "    image_bytes_data = bytes_to_json_serializable(output.getvalue())\n",
    "\n",
    "inputs = InferenceV2Inputs(\n",
    "    name=\"input-0\",\n",
    "    shape=list(image_size),\n",
    "    datatype=\"BYTES\",\n",
    "    data=[image_bytes_data]\n",
    ")\n",
    "\n",
    "inference_request_payload = InferenceV2(\n",
    "    id=str(uuid.uuid4()),\n",
    "    inputs=[inputs]\n",
    ")\n",
    "\n",
    "model_name = \"kserve-demo-model\"\n",
    "url = f\"http://localhost:8080/v2/models/{model_name}/infer\"\n",
    "request_headers = {\"Host\": f\"{model_name}.kserve.example.com\"}\n",
    "\n",
    "response = requests.post(url, data=inference_request_payload.json(), headers=request_headers)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "[3, 360, 640]"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = response.json()\n",
    "predictions['outputs'][0]['shape']\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
